{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP0142\n",
    "\n",
    "# Assignment 2: Classification with Naive Bayes & Logistic Regression\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this assignment, we will implement multiclass classification using Logistic Regression and Naive Bayes via the Scikit-learn library.\n",
    "\n",
    "In Part A of this assignment, we are going to implement Logistic Regression. First, we are going to examine how data scaling affects the performance of the classifier: We will produce a classification report and plot a confusion matrix. We are then going to use cross validation to more reliably compare the performance of the models.\n",
    "\n",
    "In Part B of this assignment, we are going to implement multiclass and Bernoulli Naive Bayes. We are going to perform feature selection and analyse the performance of the model. Finally, we are also going to look at decision boundaries.\n",
    "\n",
    "We are going to work with two datasets:\n",
    "- The 'Gene expression cancer RNA-Seq' dataset \n",
    "- The 'Zoo' dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidelines\n",
    "\n",
    "- The structure of the code is given to you and you will need to fill in the parts corresponding to each question. \n",
    "- You will have to submit the completed notebook in the Jupyter notebook format: `.ipynb`.\n",
    "- Do not modify/erase other parts of the code if you have not been given specific instructions to do so.\n",
    "- When you are asked to insert code, do so between the areas which begin:\n",
    "  \n",
    "  ````\n",
    "  ##########################################################\n",
    "  # TO_DO\n",
    "  \n",
    "  #[your code here]\n",
    "  ````\n",
    "   \n",
    "   And which end:\n",
    "   \n",
    "  ````\n",
    "  # /TO_DO\n",
    "  ##########################################################\n",
    "  ````\n",
    "\n",
    "\n",
    "- When you are asked to comment on the results you should give clear and comprehensible explanations. Write the comments in a 'Code Cell' with a sign `#` at the beginning of each row, and in the areas which begin:\n",
    "\n",
    "  `# [INSERT YOUR ANSWER HERE]`\n",
    "\n",
    "\n",
    "- Some of the code you are asked to write may generate **warning messages**. Some boilerplate code is included to suppress these warnings, but you may wish to comment it out while you are working so that you don't missing useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Please do not change the cell below, you will see a number of imports. All these packages are relevant for the assignment and it is important that you get used to them. You can find more information about them in their respective documentation. The most relevant package for this assignment is Scikit-learn:\n",
    "\n",
    "https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Standard python libraries for data and visualisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# SciKit Learn python ML Library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB, BernoulliNB\n",
    "\n",
    "# Import error metric\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# Import library for handling warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "# Functions to use\n",
    "# Decision boundary plotting \n",
    "def plot_predictions(X, y, clf):\n",
    "    h = .02  # step size in the mesh\n",
    " \n",
    "    # create a mesh to plot in\n",
    "    x_min, x_max = X.iloc[:, 0].min() - 1, X.iloc[:, 0].max() + 1\n",
    "    y_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    pylab.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel(list(X.head(0))[0])\n",
    "    plt.ylabel(list(X.head(0))[1])\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(np.arange(min(X.iloc[:, 0]), max(X.iloc[:, 0])+1, 1.0))\n",
    "    plt.yticks(np.arange(min(X.iloc[:, 1]), max(X.iloc[:, 1])+1, 1.0))\n",
    "    plt.title(clf)\n",
    "\n",
    "    plt.show\n",
    "    \n",
    "# confusion matrix plotting\n",
    "def plot_conf_matrix(conf_matrix):\n",
    "    plt.figure(figsize=(5,5))\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap=\"YlGnBu\" ,fmt='g')    \n",
    "    plt.ylabel('Actual label');\n",
    "    plt.xlabel('Predicted label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Logistic Regression [50 marks]\n",
    "\n",
    "## Gene expression dataset\n",
    "This dataset contains gene expression data from patients diagnosed with one of tumor types: BRCA, KIRC, COAD, LUAD and PRAD. Each feature corresponds to a different gene. \n",
    "\n",
    "Dataset location: https://archive.ics.uci.edu/ml/datasets/gene+expression+cancer+RNA-Seq\n",
    "\n",
    "Number of instances: 801\n",
    "\n",
    "Number of features: 20531   \n",
    "\n",
    "All of these parameters are real-valued continuous. To reduce computation time, we are going to work with the first 200 features.\n",
    "\n",
    "## Load dataset\n",
    "Please save the `data.csv` and `labels.csv` files included in the assignment zip file, which contain this data, and change the paths below to the paths leading to the location of your downloaded files. You may want to use os.chdir to change directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# PLEASE CHANGE THE FILE PATHS AS NEEDED\n",
    "\n",
    "file_path_data = \"data.csv\"\n",
    "file_path_labels = \"labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "# read the file with pandas.read_csv\n",
    "X = pd.read_csv(file_path_data, usecols=[*range(1, 201)])\n",
    "y = pd.read_csv(file_path_labels, usecols=[1]).values.ravel()\n",
    "\n",
    "label_list = [\"BRCA\", \"KIRC\", \"COAD\", \"LUAD\", \"PRAD\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis and pre-processing \n",
    "Below, we will generate histograms of the first 12 'Gene expression cancer RNA-Seq' dataset features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "figs, axs = plt.subplots(3, 4, figsize=(12, 10))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for counter in range(12):\n",
    "    col = X.columns[counter]\n",
    "    axs[counter].hist(X[col], bins=20)\n",
    "    axs[counter].set_title(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularisation makes the classifier dependent on the scale of the features. \n",
    "\n",
    "We are going to scale the features and compare the performance of Logistic Regression on unscaled and scaled dataset. \n",
    "\n",
    "### _Question 1 [10 marks]_ \n",
    "\n",
    "### _a) [3 marks]_ \n",
    "- Use `StandardScaler()` to scale the data. Save the result to a new variable (do not overwrite X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# TO_DO\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "#/TO_DO\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _b) [3 marks]_\n",
    "-  Explain how the `StandardScaler()` function changes the data, (in particular its mean and variance)? (**Hint:** You can re-run the code from the section **Data analysis and pre-processing** in order to visualise scaled values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# [INSERT YOUR ANSWER HERE]\n",
    "\n",
    "'''\n",
    "Standard scaler works by normalising the distribution of data to a mean = 0 and a standard deviation of 1. As the distribution itself is normalised,\n",
    "the structure of data is maintained as relationships between the individual data points are preserved, despite the data values themselves change.\n",
    "The standard score of a sample x is calculated as: z = (x - u) / s where 'u' is the mean and 's' is the standard deviation.\n",
    "\n",
    "First you calculate the mean value for each feature in the dataset and then subtract this mean from every individual data point within that feature.\n",
    "This new mean of the standardised data is centered around 0.\n",
    "\n",
    "Next, you divide the centred data by the standard deviation to scale the variance of each feature to 1. This is because variance is defined as the \n",
    "average of the squared differences from the mean, and by dividing by the standard deviation, you're normalizing these squared differences. The \n",
    "standardised data achieve a unit variance of 1. Since the data was centred around zero in the first step, and is now scaled by the standard deviation, \n",
    "the result is a feature with a variance of 1. It makes sense as variance can be calculated by the square of standard deviation (in a standard data \n",
    "distribution), the variance is therefore 1.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _c) [4 marks]_\n",
    "- `LogisticRegression()` uses $\\ell_2$ regularisation as default. Briefly explain the effect of such a regulariser. Furthermore, briefly explain why data scaling might be a useful pre-processing step before the application of such a regulariser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# [INSERT YOUR ANSWER HERE]\n",
    "'''\n",
    "Regularisation provides a penalising term to a model of increasing complexity. It controls the amount of shrinkage, pushing weight coefficients towards \n",
    "0 in order to reduce the loss function. \n",
    "\n",
    "The effect of a regulariser such as the l2 norm controls the effect of overfitting. For example, in the case of perfectly separable data, the gradient \n",
    "descent approach may provide an infinite solution and therefore may need a regularised term for it to be finite and unique when calculating MLE. \n",
    "It penalises large coefficients and therefore the model is discouraged from depending too much on any one feature and lowers the complexity of the model as a whole.\n",
    "\n",
    "Ridge regulariser L2 also has the benefit of ensuring that gradient descent leads to a globally unique solution for any input data. It means that\n",
    "the model is more robust and therefore is less likely to converge to overfitting solutions, making the model more resistant to minor changes in the training data.\n",
    "\n",
    "Data scaling might be a useful pre-processing step before applying a regulariser because it normalises features to a similar scale. Parameters\n",
    "will have values in the same range, which eliminates potential biases and distortions that maybe caused by having test and training data of different scales,\n",
    "and therefore remove improper/disproportionate penalisation when parameters are large (this may be the case prior to data scaling). It would mean that each feature\n",
    "can contribute equally to the model's predictions thanks to this uniform approach. Improper penalisation leads to increased generalisability of models, which is undesired \n",
    "when we want to optimise a model for a particular dataset.\n",
    "\n",
    "Another benefit of scaling is that it ensures smoother optimisation through improved convergence to minima. Intuitively, processes such as gradient descent is sped up in smaller \n",
    "ranges of values compared to values of a larger range.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier performance analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix is a table used for the evaluation of classification models. The x axis represents predicted labels while the y axis represents actual labels. Each cell indicates the sum of instances assigned to a particular combination of these labels. Diagonal values represents correctly classified instances.  \n",
    "\n",
    "### _Question 2 [20 marks]_\n",
    "\n",
    "### _a) [5 marks]_ \n",
    "- Create training and testing datasets for the unscaled and scaled data (set `random_state=42` and `train_size=0.7` when making your split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "lg = LogisticRegression(solver = \"lbfgs\", multi_class = \"multinomial\", max_iter = 5000)\n",
    "lg_scaled = LogisticRegression(solver = \"lbfgs\", multi_class = \"multinomial\", max_iter = 5000)\n",
    "\n",
    "#######################################################\n",
    "# TO_DO\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y, train_size=0.7, random_state=42)\n",
    "\n",
    "\n",
    "#/TO_DO\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _b) [5 marks]_ \n",
    "- Fit `LogisticRegression()` to the unscaled and scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    lg.fit(X_train, y_train)\n",
    "    lg_scaled.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "\n",
    "    #/TO_DO\n",
    "    #######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _c) [5 marks]_ \n",
    "- Plot confusion matrices for the scaled and unscaled data using Scikit-learn `confusion_matrix()` and the `plot_conf_matrix()` function defined for you at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# TO_DO\n",
    "\n",
    "y_pred = lg.predict(X_test)\n",
    "y_pred_scaled = lg_scaled.predict(X_test_scaled)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "conf_matrix_scaled = confusion_matrix(y_test_scaled, y_pred_scaled)\n",
    "\n",
    "plot_conf_matrix(conf_matrix)\n",
    "plot_conf_matrix(conf_matrix_scaled)\n",
    "                 \n",
    "\n",
    "#/TO_DO\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _d) [5 marks]_ \n",
    "- Print a classification report using scikit-learn `classification_report()` function. You can use `target_names = label_list` to include labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    print(\"Classification Report (Unscaled Data):\")\n",
    "    print(classification_report(y_test, y_pred, target_names=[\"BRCA\", \"KIRC\", \"COAD\", \"LUAD\", \"PRAD\"]))\n",
    "\n",
    "    print(\"\\nClassification Report (Scaled Data):\")\n",
    "    print(classification_report(y_test_scaled, y_pred_scaled, target_names=[\"BRCA\", \"KIRC\", \"COAD\", \"LUAD\", \"PRAD\"]))\n",
    "\n",
    "\n",
    "    #/TO_DO\n",
    "    #######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation\n",
    "In Scikit-learn, `StratifiedKFold()` splits the data into $k$ different folds.  \n",
    "`cross_val_score()` then uses these folds to run the classifier multiple times and collect multiple accuracy scores.   \n",
    "\n",
    "### _Question 3 [20 marks]_\n",
    "\n",
    "### _a) [5 marks]_ \n",
    "- Split data using `StratifiedKFold()`. Set `n_splits = 10`, `shuffle = True`, and `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "       X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "       y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \n",
    "    #/TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _b) [5 marks]_ \n",
    "- Calculate cross validation scores using `cross_val_score()`. Call the variables storing these scores `lg_scores` and `lg_scaled_scores` (for consistency with plotting done for you in the subsequent section). (**Hint:** `cv` is equal to the output of `StratifiedKFold()`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    lg_scores = cross_val_score(lg, X, y, cv=skf)\n",
    "    lg_scaled_scores = cross_val_score(lg_scaled, X_scaled, y, cv=skf)\n",
    "    \n",
    "    #/TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _c) [5 marks]_ \n",
    "- Calculate and print the mean of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    print(f\"Mean cross-validation score (Unscaled Data): {lg_scores.mean()}\")\n",
    "    print(f\"Mean cross-validation score (Scaled Data): {lg_scaled_scores.mean()}\")\n",
    "\n",
    "    \n",
    "    #/TO_DO\n",
    "    ########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _d) [5 marks]_ \n",
    "- Unlike vanilla `KFold()`, `StratifiedKFold()` aims to preserve the proportion of examples belonging to each class in each split. Does `StratifiedKFold()` make each data split balanced if the whole dataset is not balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# [INSERT YOUR ANSWER HERE]\n",
    "'''\n",
    "If the dataset is balanced, StratifiedFold() will keep the balance across each fold with each class being equally represented. However if the dataset is not \n",
    "balanced, then the function does not balance the number of examples from each class. It ensures that the proportion of each class within each fold mirrors the \n",
    "overall dataset's proportions rather than balancing the number of samples from each class in each fold. This approach is particularly beneficial for dealing with \n",
    "imbalanced datasets since it eliminates the danger of removing minority classes from some folds, which is a potential concern with vanilla KFold() splitting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise the scores using a box plot. It highlights the lower and upper quartiles, and \"whiskers\" showing the extent of the scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot([1]*10, lg_scores, \".\")\n",
    "plt.plot([2]*10, lg_scaled_scores, \".\")\n",
    "plt.boxplot([lg_scores, lg_scaled_scores], labels=(\"logistic regression\",\"logistic regression w/ scaling\"))\n",
    "plt.ylabel(\"Accuracy\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Naive Bayes [50 marks]\n",
    "\n",
    "Please note that we are still working with the 'Gene expression cancer RNA-Seq' dataset loaded in Part A.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing correlated features\n",
    "Feature independence is an assumption of Naive Bayes. Naive Bayes is particularly sensitive to feature correlations which can lead to overfitting. Based on data alone, we cannot test if features are truly independent, but we can exclude correlated features.\n",
    "Below, we test if features are correlated.\n",
    "\n",
    "### _Question 4 [10 marks]_\n",
    "Drop features with correlation above 0.75.\n",
    "\n",
    "**Hint:** see what `to_drop` returns, then use it as an argument in the *pandas* `drop()` function with `axis=1` and `inplace=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = X.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.75\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\n",
    "\n",
    "#######################################################\n",
    "# TO_DO\n",
    "\n",
    "X.drop(columns=to_drop, axis=1, inplace=True)\n",
    "# the code here uses pd drop to drop the data that are correlated. \n",
    "# Here an arbitrary value of 0.75 was used to show correlation in these genes\n",
    "\n",
    "\n",
    "#/TO_DO\n",
    "#######################################################\n",
    "\n",
    "print(\"Correlated features dropped: \")\n",
    "print(*to_drop, sep = \", \") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Recursive feature elimination\n",
    "Lets go further and select the 5 most important features. Recursive Feature Elimination (RFE) is designed to select features by recursively considering smaller and smaller sets of features.  \n",
    "\n",
    "### _Question 5 [10 marks]_\n",
    "\n",
    "### _a) [5 marks]_ \n",
    "- Use the `RFE()` function in Scikit-learn to select features. (**Hint:** The `MultinomialNB` and `BernoulliNB` models do not have a `coef_` attribute, but the attribute `feature_log_prob_` can serve a similar purpose. Check the Scikit-learn documentation and examples for more information.)\n",
    "\n",
    "### _b) [5 marks]_ \n",
    "- After selecting features to eliminate, use the `support_` attribute as a mask to select the right columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "#######################################################\n",
    "# TO_DO\n",
    "# [your code here]\n",
    "# im pretty sure the normal approach is to split the data into test and train\n",
    "# then fit the data to the classification\n",
    "from operator import attrgetter\n",
    "\n",
    "nb.fit(X, y) \n",
    "rfe = RFE(estimator=nb, \n",
    "          n_features_to_select=5, step=1,\n",
    "          importance_getter=attrgetter(\"feature_log_prob_\")) # here i select the 5 most important features\n",
    "rfe.fit(X, y)\n",
    "selected_features = rfe.support_\n",
    "selected_feature_indices = np.where(selected_features)[0] # obtain indices of selected features\n",
    "selected_features = X.columns[selected_feature_indices] # get column names of selected features\n",
    "print(f\"Selected features: {', '.join(map(str, selected_features))}\")\n",
    "\n",
    "#/TO_DO\n",
    "#######################################################\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "nb.fit(X_train, y_train)\n",
    "nb_predict = nb.predict(X_test)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "    print(classification_report(y_test, nb_predict, target_names=label_list))\n",
    "\n",
    "nb_confusion = confusion_matrix(y_test, nb_predict)\n",
    "plot_conf_matrix(nb_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to switch to a different dataset.\n",
    "### Zoo dataset \n",
    "This is a simple dataset which classifies animals into 7 categories. \n",
    "\n",
    "Dataset location: https://archive.ics.uci.edu/ml/datasets/Zoo  \n",
    "\n",
    "Number of instances: 210\n",
    "\n",
    "Number of features: 17\n",
    "\n",
    "Attribute Information:  \n",
    "1. animal name: Unique for each instance   \n",
    "2. hair:\tBoolean   \n",
    "3. feathers:\tBoolean   \n",
    "4. eggs:\tBoolean   \n",
    "5. milk:\tBoolean   \n",
    "6. airborne:\tBoolean   \n",
    "7. aquatic:\tBoolean   \n",
    "8. predator:\tBoolean   \n",
    "9. toothed:\tBoolean   \n",
    "10. backbone:\tBoolean   \n",
    "11. breathes:\tBoolean   \n",
    "12. venomous:\tBoolean   \n",
    "13. fins:\tBoolean   \n",
    "14. legs:\tNumeric (set of values: {0,2,4,5,6,8})   \n",
    "15. tail:\tBoolean   \n",
    "16. domestic:\tBoolean   \n",
    "17. catsize:\tBoolean   \n",
    "18. type:\tNumeric (integer values in range [1,7])  \n",
    "\n",
    "All of these parameters are discrete-valued.\n",
    "  \n",
    "### Load dataset\n",
    "Please save the 'zoo.csv' file included in the assignement zip file, which contains a subset of this data, and change the paths below to the paths leading  to the location of your downloaded files. You may want to use `os.chdir` to change directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# PLEASE CHANGE THE FILE PATHS AS NEEDED\n",
    "\n",
    "file_path_data_zoo = \"zoo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "# read the file with pandas.read_csv\n",
    "data_zoo = pd.read_csv(file_path_data_zoo)\n",
    "\n",
    "# because the file does not contain header information, we manually add headers of the dataset\n",
    "data_zoo.columns = [\"animal name\", \"hair\", \"feathers\",\"eggs\", \n",
    "                \"milk\", \"airborne\",\"aquatic\", \"predator\", \"toothed\", \"backbone\", \n",
    "                    \"breathless\", \"venomous\", \"fins\", \"legs\", \"tail\", \"domestic\", \"catsize\", \"class\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assign columns 2 to 18 (everything other than animal name and class) to variable `X_zoo`. Remember that indexing starts at 0.  \n",
    "We assign the \"class\" column to variable `y`.  \n",
    "We then split `X` and `y` into train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "X_zoo = data_zoo.iloc[:,1:17]\n",
    "y_zoo = data_zoo[\"class\"]\n",
    "X_zoo_train, X_zoo_test, y_zoo_train, y_zoo_test = train_test_split(X_zoo, y_zoo, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we create a test dataset which contains only animals with 4 or fewer legs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "X_drop_train = X_zoo_train.drop(X_zoo_train[X_zoo_train[\"legs\"]>4].index)\n",
    "y_drop_train = y_zoo_train.drop(X_zoo_train[X_zoo_train[\"legs\"]>4].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an instance of a multinomial Naive Bayes classifier. We train `nb` on `X_drop_train`, and test it on `X_zoo_test`.  \n",
    "You should get a warning message suggesting that the value of alpha is automatically overwritten. (Note that this message is not suppressed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#PLEASE DO NOT CHANGE THIS CELL\n",
    "\n",
    "nb = MultinomialNB(alpha =0)\n",
    "nb.fit(X_drop_train, y_drop_train)\n",
    "nb_predict_train = nb.predict(X_drop_train)\n",
    "nb_predict_test = nb.predict(X_zoo_test)\n",
    "print(accuracy_score(nb_predict_train, y_drop_train))\n",
    "print(accuracy_score(nb_predict_test, y_zoo_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Question 6 [10 marks]_   \n",
    "Please comment on what the alpha parameter does.\n",
    "**Hints:**  \n",
    "- Think what Naive Bayes does when encountering a discrete feature value of which is absent in the train dataset but which is present in the test dataset. What probability estimate would be associated with it?  \n",
    "- See: Scikit-learn `MultinomialNB` documentation, in particular the description of the alpha parameter https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "### [INSERT YOUR ANSWER HERE]\n",
    "'''\n",
    "The alpha parameter demonstrates the stength of smoothing in NB in a process called additive\n",
    "(laplace) smoothing, mathematically derived from the Dirichelet distributions.\n",
    "\n",
    "The parameter prevents zero probabilities, which can be problematic in this scenario\n",
    "when discrete feature values are absent in the train dataset, whilst being present in the test \n",
    "dataset. This is because NB assigns a 0 to a probability estimate such as MLE (maximum \n",
    "log-likelihood estimator). This case of overfitting leads to Py(k|x) being undefined. \n",
    "\n",
    "Additive smoothing works by adding the alpha parameter uniformly adding instances to the data. \n",
    "The alpha parameter is added to the numerator of the probability estimate MLE \n",
    "(number of training points X in animal \"class\" + alpha parameter) \n",
    "as well as the denominator of the MLE\n",
    "(number of training points in animal \"class\" + alpha parameter x number of features in data).\n",
    "'''\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going  to repeat the above process using Naive Bayes for features with the Bernoulli distribution (`BernoulliNB()` in scikit-learn). We are particularly interested in interpreting decision boundaries of Bernoulli Naive Bayes.\n",
    "\n",
    "### _Question 7 [20 marks]_\n",
    "\n",
    "### _a) [2 marks]_ \n",
    "- Use recursive feature elimination (RFE) is to select only 2 features (**Note:** in real life cases, you are likely to use this approach to select multiple rather than just two features. In this case we are asking you to analyse the outcome, and 2 features allows us to visualise the decision boundary more easily.)\n",
    "\n",
    "### _b) [1 mark]_ \n",
    "- After selecting the features to eliminate, use the `support_` attribute as a mask to select the right columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "bnb = BernoulliNB()\n",
    "\n",
    "#######################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "from operator import attrgetter\n",
    "\n",
    "bnb.fit(X_zoo,y_zoo)\n",
    "estimator = bnb\n",
    "rfe_zoo = RFE(estimator, \n",
    "          n_features_to_select=2, step=1,\n",
    "          importance_getter=attrgetter(\"feature_log_prob_\")) # the 2 most important features are selected\n",
    "rfe_zoo.fit(X_zoo,y_zoo)\n",
    "\n",
    "zoo_selected_features = rfe_zoo.support_\n",
    "zoo_selected_features_indices = np.where(zoo_selected_features)[0] # obtain indices of selected features\n",
    "X_zoo_selected_features = X_zoo.columns[zoo_selected_features_indices] # get column names of selected features\n",
    "print(f\"Selected features: {', '.join(map(str, X_zoo_selected_features))}\")\n",
    "\n",
    "#/TO_DO\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _c) [2 marks]_ \n",
    "- Split data into train and test sets (set `random_state=42`).\n",
    "\n",
    "### _d) [1 mark]_ \n",
    "- Fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "X_zoo_train, X_zoo_test, y_zoo_train, y_zoo_test = train_test_split(X_zoo, y_zoo, test_size=0.2, random_state=42)\n",
    "bnb.fit(X_zoo_train, y_zoo_train)\n",
    "\n",
    "#/TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _e) [2 marks]_ \n",
    "- Create a Confusion Matrix using the `plot_conf_matrix()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# TO_DO\n",
    "#[your code here]\n",
    "bnb_predict = bnb.predict(X_zoo_test)\n",
    "bnb_confusion = confusion_matrix(y_zoo_test, bnb_predict)\n",
    "plot_conf_matrix(bnb_confusion)\n",
    "\n",
    "#/TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _f) [2 marks]_ \n",
    "- Plot decision boundaries using the `plot_predictions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "with warnings.catch_warnings():\n",
    "    \n",
    "    ########################################################\n",
    "    # TO_DO\n",
    "\n",
    "    # NB: you may wish to comment out this filter\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    #[your code here]\n",
    "    print(X_zoo_train.shape)\n",
    "    # output provided was (80,16)\n",
    "    \n",
    "    X_zoo_train_selected_features = X_zoo_train.iloc[:, zoo_selected_features_indices]\n",
    "    \n",
    "    # verify shape of X_zoo_train_selected_features\n",
    "    print(X_zoo_train_selected_features.shape)\n",
    "    # output provided was (80,2)\n",
    "    bnb.fit(X_zoo_train_selected_features, y_zoo_train)\n",
    "\n",
    "    plot_predictions(X_zoo_train_selected_features, y_zoo_train, bnb)\n",
    "    \n",
    "\n",
    "    #/TO_DO\n",
    "##########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _g) [10 marks]_ \n",
    "- Interpret the decision boundaries: Recall the shapes of decision boundaries you have seen in classes - were they straight and crossing at right angles? Why is this the case when using `BernoulliNB()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "### [INSERT YOUR ANSWER HERE]\n",
    "'''\n",
    "The decision boundaries shown above are straight lines and cross at right angles. The 2D decision boundaries formed from different classification \n",
    "techniques are not parallel to the axes such as in logistic regression or are curved/quadratic in some generative classifications. These shapes \n",
    "arise due to the conditional probabilities of these features, more particularly the diverse range of interactions these data values have.\n",
    "\n",
    "The decision boundaries above are parallel to the axes because of the assumption of binary values, which are then modelled using Bernoulli \n",
    "distributions. The boundaries are much simpler due to values being confined to either a 0 or a 1. It makes sense that it is modelled this \n",
    "way as the data provided are booleans that take up either True or False values. Since these boundaries are straight lines parallel to the \n",
    "axes, we know that the decisions are based on whether a particular feature is above or below a certain threshold. \n",
    "\n",
    "Assuming that 0 is false and 1 is true, above shows a 3 points on the graph. The dataset only shows 3 potential options that animals either:\n",
    "1. does not produce milk and does not have feathers\n",
    "2. produces milk but does not have feathers\n",
    "3. does not produce milk but does have feathers.\n",
    "\n",
    "The boundaries does not suggest the possibility of an animal that both produces milk and has feathers.\n",
    "\n",
    "'''\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment '/opt/homebrew/bin/python3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
